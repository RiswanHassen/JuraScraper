#!/usr/bin/env python3
"""
Jurascraper â€“ Version 3.0

Ein Open-Source-Tool zum automatisierten Herunterladen juristischer PDF-Dokumente von gesetze-im-internet.de.

Dieses Tool durchsucht systematisch die Teillisten Aâ€“Z und 0â€“9, filtert PDF-Links nach optionalen Kriterien
(Start, Enthalten, Ende des Dateinamens) und lÃ¤dt sie in ein angegebenes Zielverzeichnis herunter.

Â© 2025 â€“ MIT Lizenz
"""

# â€”â€”â€” Importierte Bibliotheken (Kern) â€”â€”â€”
import os
import sys
import argparse
import subprocess
import importlib
import shutil
from urllib.parse import urljoin
from string import ascii_uppercase, digits
from concurrent.futures import ThreadPoolExecutor

# â€”â€”â€” Python-Version & automatische Modulinstallation â€”â€”â€”
if sys.version_info < (3, 8):
    print("âŒ Python 3.8 oder hÃ¶her ist erforderlich. Deine Version:", sys.version)
    sys.exit(1)

REQUIRED_MODULES = ["requests", "bs4"]
for module in REQUIRED_MODULES:
    try:
        importlib.import_module(module)
    except ImportError:
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", module], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            # Automatische Installation fehlgeschlagen: Hinweise fÃ¼r manuelle Installation
            print("âŒ Automatische Installation von '{}' fehlgeschlagen.".format(module))
            print("ðŸ’¡ Du kannst das Modul manuell installieren mit:")
            print("   pip install {}".format(module))
            sys.exit(1)

# ggf. automatisch mit 'python3' neu starten wenn nur 'python3' existiert
if shutil.which("python3") and not shutil.which("python") and not sys.executable.endswith("python3"):
    os.execv(shutil.which("python3"), ["python3"] + sys.argv)

# â€”â€”â€” Jetzt die restlichen Module importieren â€”â€”â€”
import requests
from bs4 import BeautifulSoup

# â€”â€”â€” CLI-Argumente parsen â€”â€”â€”
parser = argparse.ArgumentParser(
    description="PDF-Scraper fÃ¼r juristische Inhalte von gesetze-im-internet.de"
)
parser.add_argument("--output_dir", type=str, required=True, help="Zielverzeichnis fÃ¼r heruntergeladene PDFs")
parser.add_argument("--limit", type=int, default=0, help="Maximale Anzahl herunterzuladender PDFs (0 = unbegrenzt)")
parser.add_argument("--startswith", "-x", type=str, default="", help="Nur Dateien mit diesem PrÃ¤fix im Dateinamen")
parser.add_argument("--contains", "-y", type=str, default="", help="Nur Dateien, die diesen Teilstring enthalten")
parser.add_argument("--endswith", "-z", type=str, default="", help="Nur Dateien mit diesem Suffix")
parser.add_argument("--preview", "-p", action="store_true", help="Nur Treffer anzeigen, aber nicht herunterladen")
parser.add_argument("--threads", type=int, default=1, help="Anzahl paralleler Downloads (Standard: 1). âš ï¸ Nicht Ã¼bertreiben â€“ zu viele gleichzeitige Verbindungen kÃ¶nnen vom Server blockiert werden!")
args = parser.parse_args()

# â€”â€”â€” Verzeichnis vorbereiten â€”â€”â€”
os.makedirs(args.output_dir, exist_ok=True)

# â€”â€”â€” Ziel-URLs generieren (Teillisten A-Z + 0â€“9) â€”â€”â€”
TEILLISTEN = [f"https://www.gesetze-im-internet.de/Teilliste_{c}.html" for c in ascii_uppercase + digits]

# â€”â€”â€” PDF-Links sammeln â€”â€”â€”
print("\U0001f4e1 Durchsuche Teillisten auf PDF-Links...")
pdf_links = []

for url in TEILLISTEN:
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.lower().endswith(".pdf"):
                full_url = urljoin(url, href)
                pdf_links.append(full_url)
    except Exception as e:
        print(f"âŒ Fehler beim Laden von {url}: {e}")

print(f"\nðŸ” Gefunden: {len(pdf_links)} PDF-Links insgesamt")

# â€”â€”â€” Filter anwenden â€”â€”â€”
filtered_links = []
for link in pdf_links:
    name = os.path.basename(link).lower()
    if args.startswith and not name.startswith(args.startswith.lower()):
        continue
    if args.contains and args.contains.lower() not in name:
        continue
    if args.endswith and not name.endswith(args.endswith.lower()):
        continue
    filtered_links.append(link)

print(f"âœ… Nach Filterung: {len(filtered_links)} passende Links")

# â€”â€”â€” Vorschau-Modus â€”â€”â€”
if args.preview:
    print("\nðŸ“ Vorschau-Modus aktiviert. Diese Dateien wÃ¼rden gespeichert werden:")
    for link in filtered_links:
        print("âž¡ï¸", os.path.basename(link))
    sys.exit(0)

# â€”â€”â€” Download-Funktion â€”â€”â€”
def download(link):
    global count
    filename = os.path.basename(link)
    filepath = os.path.join(args.output_dir, filename)

    if os.path.exists(filepath):
        print(f"âš ï¸ Datei existiert bereits: {filename}")
        return

    try:
        with requests.get(link, stream=True) as r:
            r.raise_for_status()
            with open(filepath, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"âœ… Gespeichert: {filename}")
    except Exception as e:
        print(f"âŒ Fehler bei {filename}: {e}")

# â€”â€”â€” Paralleles Herunterladen â€”â€”â€”
count = 0
print(f"\nâ¬‡ï¸ Starte Download mit {args.threads} Thread(s)...")

if args.limit:
    filtered_links = filtered_links[:args.limit]

with ThreadPoolExecutor(max_workers=args.threads) as executor:
    executor.map(download, filtered_links)

# â€”â€”â€” (Kommendes Feature) Asynchrone Download-Logik mit asyncio â€”â€”â€”
# Hinweis: Dieses Feature ist fÃ¼r zukÃ¼nftige Versionen vorgesehen.
# Es ermÃ¶glicht effizientere Downloads bei sehr vielen kleinen Dateien und reduziert Ressourcenverbrauch.
#
# import aiohttp
# import asyncio
#
# async def async_download(session, url, output_path):
#     async with session.get(url) as resp:
#         if resp.status == 200:
#             with open(output_path, 'wb') as f:
#                 f.write(await resp.read())
#
# async def async_main(urls):
#     async with aiohttp.ClientSession() as session:
#         tasks = []
#         for url in urls:
#             path = os.path.join(args.output_dir, os.path.basename(url))
#             tasks.append(async_download(session, url, path))
#         await asyncio.gather(*tasks)
#
# asyncio.run(async_main(filtered_links))

# â€”â€”â€” Abschlussmeldung â€”â€”â€”
print(f"\nðŸ“„ Fertig: {len(filtered_links)} PDF-Dateien verarbeitet in '{args.output_dir}'")
